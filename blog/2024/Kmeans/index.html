<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Kmeans Clustering Algorithm with CUDA | PoJen Wang </title> <meta name="author" content="PoJen Wang"> <meta name="description" content="Kmeans Clustering Algorithm with CUDA and CUDA Thrust API"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nez0b.github.io/blog/2024/Kmeans/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Kmeans Clustering Algorithm with CUDA",
            "description": "Kmeans Clustering Algorithm with CUDA and CUDA Thrust API",
            "published": "October 01, 2024",
            "authors": [
              
              {
                "author": "PoJen Wang",
                "authorURL": "https://nez0b.github.io",
                "affiliations": [
                  {
                    "name": "IBM Q-hub, National Taiwan University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">PoJen</span> Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Kmeans Clustering Algorithm with CUDA</h1> <p>Kmeans Clustering Algorithm with CUDA and CUDA Thrust API</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#cuda-implementation">CUDA Implementation</a> </div> <ul> <li> <a href="#basic-cuda-implementation">Basic CUDA implementation</a> </li> <li> <a href="#cuda-shared-memory">CUDA shared memory</a> </li> <li> <a href="#cuda-thrust">CUDA Thrust</a> </li> </ul> <div> <a href="#comparison-between-different-implementations">Comparison between different implementations</a> </div> <div> <a href="#conclusions">Conclusions</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>K-means <d-cite key="wiki"></d-cite>is a widely used unsupervised machine learning algorithm for clustering data points into K distinct groups based on their similarities. The algorithm aims to minimize the sum of squared distances between data points and their assigned cluster centroids. K-means has found applications in various fields, including image processing, customer segmentation, and data compression.</p> <p>In the general framework of the k-means algorithm, we are given a dataset $\mathbf{X} = (\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_n})$, where each $\mathbf{x_i}$ represents a $d$-dimensional point in a vector space. The goal is to partition this dataset into $k$ distinct clusters $\mathbf{S} = {\mathbf{S_1}, \mathbf{S_2}, \dots, \mathbf{S_k}}$ such that points within each cluster are more similar to each other than to those in different clusters. This is achieved by solving the following optimization problem <d-footnote>However, solving this optimization problem exactly is NP-hard, which necessitates the use of approximation algorithms in practice. One common approximation method is Lloyd’s algorithm, developed by Stuart Lloyd in 1957. This iterative algorithm is what most people refer to when discussing k-means</d-footnote></p> <p>\(\underset{\mathbf{S}}{\text{arg min}} \sum_{i=1}^k \sum_{\mathbf{x} \in \mathbf{S_i}} ||\mathbf{x} - \mathbf{\mu_i}||^2,\) where $\mathbf{\mu_i}$ denotes the centroid of cluster $\mathbf{S_i}$.</p> <h3 id="k-means-algorithm-overview">K-means Algorithm Overview</h3> <p>The standard K-means algorithm follows these steps:</p> <ul> <li>Initialization: Randomly select K points as initial cluster centroids.</li> <li>Assignment: Assign each data point to the nearest centroid based on Euclidean distance.</li> <li>Update: Recalculate the centroids of each cluster by computing the mean of all points assigned to that cluster.</li> <li>Iteration: Repeat steps 2 and 3 until convergence or a maximum number of iterations is reached.</li> </ul> <h2 id="cuda-implementation">CUDA Implementation</h2> <h3 id="basic-cuda-implementation">Basic CUDA implementation</h3> <p>Our CUDA implementation of Kmeans algorithm follows from the the paradigm from the previous section. In the second step, “Assignment” part, we could leverage the thousands of CUDA cores to calculate the nearest center for each point simultaneously. The kernel function is outline below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">__global__</span> <span class="kt">void</span> 
<span class="nf">mapPointsToCenters</span><span class="p">(</span>
    <span class="n">point</span> <span class="o">*</span><span class="n">d_Points</span><span class="p">,</span>
    <span class="n">point</span> <span class="o">*</span><span class="n">d_Centers</span><span class="p">,</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">d_Labels</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">numPoints</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">numCenters</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span><span class="p">(</span><span class="n">idx</span> <span class="o">&gt;=</span> <span class="n">numPoints</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
    <span class="kt">float</span> <span class="n">min_dist</span> <span class="o">=</span> <span class="n">FLT_MAX</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">min_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">numCenters</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">dist</span> <span class="o">=</span> <span class="n">distance</span><span class="p">(</span><span class="n">d_Points</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">d_Centers</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
        <span class="k">if</span><span class="p">(</span><span class="n">dist</span> <span class="o">&lt;</span> <span class="n">min_dist</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">min_idx</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
            <span class="n">min_dist</span> <span class="o">=</span> <span class="n">dist</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">d_Labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_idx</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <p>Next, we reset the centroids to zero in preparation for recalculating them in the following step. Each point is then assigned to its nearest centroid. Afterward, we implement a kernel to sum all points sharing the same label and record the number of points assigned to each label in the variable <em>d_ClusterCounts</em>. Finally, we divide the cumulative sum of vectors for each label by <em>d_ClusterCounts</em> to compute the new centroids.</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre></td> <td class="rouge-code"><pre><span class="n">__global__</span> <span class="kt">void</span> 
<span class="nf">accumulateCenters</span><span class="p">(</span>
    <span class="n">point</span> <span class="o">*</span><span class="n">d_Points</span><span class="p">,</span>
    <span class="n">point</span> <span class="o">*</span><span class="n">d_Centers</span><span class="p">,</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">d_ClusterCounts</span><span class="p">,</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">d_Labels</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">numPoints</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">numCenters</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="k">if</span><span class="p">(</span><span class="n">idx</span> <span class="o">&gt;=</span> <span class="n">numPoints</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">clusterid</span> <span class="o">=</span> <span class="n">d_Labels</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
        <span class="kt">int</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">d_Points</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">size</span><span class="p">;</span>
	<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">dim</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span> 
		<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_Centers</span><span class="p">[</span><span class="n">clusterid</span><span class="p">].</span><span class="n">entries</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">d_Points</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="n">entries</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
	<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_ClusterCounts</span><span class="p">[</span><span class="n">clusterid</span><span class="p">],</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">__global__</span> <span class="kt">void</span> 
<span class="nf">updateCenters</span><span class="p">(</span>
    <span class="n">point</span> <span class="o">*</span><span class="n">d_Centers</span><span class="p">,</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">d_ClusterCounts</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">numCenters</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="k">if</span><span class="p">(</span><span class="n">idx</span> <span class="o">&gt;=</span> <span class="n">numCenters</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
	<span class="n">d_Centers</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">/=</span> <span class="n">d_ClusterCounts</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="p">}</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <h3 id="cuda-shared-memory">CUDA shared memory</h3> <p>In the previous basic CUDA implementation, all the variables are stored and accessed from GPU’s global memory. We could improve our implementation by using GPU’s shared memory. CUDA’s shared memory is a special type of on-chip memory that allows threads within the same block to share data and communicate efficiently. It is much faster than global memory (which resides off-chip) but is limited in size (typically around 48 KB per block). Shared memory is accessible to all threads within a block, and it provides a mechanism for threads to collaborate by reading and writing to a common memory space, enabling data reuse and reducing the need for costly global memory access.</p> <p>In the second step, where we map each point to its nearest centroid, we could leverage the shared memory and stored the coordinate of the centroids in the shared memory so that it could be accessed much faster. Below is my implementation:</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre></td> <td class="rouge-code"><pre><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">mapPointsToCenters_shmem</span><span class="p">(</span>
<span class="kt">int</span> <span class="n">DIM</span><span class="p">,</span>
    <span class="kt">double</span><span class="o">*</span> <span class="n">d_Points</span><span class="p">,</span>
    <span class="kt">double</span><span class="o">*</span> <span class="n">d_Centers</span><span class="p">,</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">d_Labels</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">numPoints</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">numCenters</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">shmemSize</span> <span class="o">=</span> <span class="n">DIM</span> <span class="o">*</span> <span class="n">numCenters</span><span class="p">;</span>
    <span class="c1">//----------LOAD CENTROIDS TO SHARED MEMORY ----------------------------</span>
    <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">double</span> <span class="n">d_Centers_shmem</span><span class="p">[];</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&gt;=</span> <span class="n">numPoints</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>

    <span class="c1">// Load centers into shared memory for each block</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">shmemSize</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">d_Centers_shmem</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_Centers</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>  <span class="c1">// Copy from global memory to shared memory</span>
    <span class="p">}</span>
    <span class="c1">//----------LOAD CENTROIDS TO SHARED MEMORY ----------------------------</span>
    <span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">// Ensure all centers are loaded before proceeding</span>

    <span class="c1">// Find nearest center</span>
    <span class="kt">double</span> <span class="n">min_dist</span> <span class="o">=</span> <span class="n">DBL_MAX</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">min_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numCenters</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
	<span class="kt">double</span> <span class="n">dist</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">DIM</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
		<span class="kt">double</span> <span class="n">diff</span> <span class="o">=</span> <span class="n">d_Points</span><span class="p">[</span><span class="n">idx</span> <span class="o">*</span> <span class="n">DIM</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">d_Centers_shmem</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">DIM</span> <span class="o">+</span> <span class="n">j</span><span class="p">];</span>
            <span class="n">dist</span> <span class="o">+=</span> <span class="n">diff</span> <span class="o">*</span> <span class="n">diff</span><span class="p">;</span>
        <span class="p">}</span>
	<span class="n">dist</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">dist</span><span class="p">);</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">dist</span> <span class="o">&lt;</span> <span class="n">min_dist</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">min_dist</span> <span class="o">=</span> <span class="n">dist</span><span class="p">;</span>
            <span class="n">min_idx</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">d_Labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_idx</span><span class="p">;</span>
<span class="p">}</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <h3 id="cuda-thrust">CUDA Thrust</h3> <p>The CUDA Thrust library is a high-level, C++-based parallel programming library designed to simplify GPU programming using NVIDIA’s CUDA architecture. It provides a set of STL (Standard Template Library)-like data structures and algorithms optimized for parallel execution on CUDA-enabled devices. Thrust abstracts the complexities of CUDA, allowing developers to write efficient parallel code with minimal knowledge of low-level CUDA programming. Below we highlight the key points.</p> <h4 id="calculate-distance-between-d-dimensional-vectors">Calculate distance between d-dimensional vectors</h4> <p>Given two d-dimension vectors, we could use the Thrust API to calcualte their euclidean distance as:</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td> <td class="rouge-code"><pre><span class="kt">double</span> <span class="nf">distance</span><span class="p">(</span><span class="k">const</span> <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&amp;</span> <span class="n">centers</span><span class="p">,</span> 
                <span class="k">const</span> <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&amp;</span> <span class="n">old_centers</span><span class="p">,</span> 
                <span class="kt">int</span> <span class="n">DIM</span><span class="p">,</span> 
                <span class="kt">int</span> <span class="n">numCenters</span><span class="p">,</span> 
                <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&amp;</span> <span class="n">d_center_idx</span><span class="p">,</span>
                <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">squared_diffs</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">squared_diff_centers</span><span class="p">(</span><span class="n">numCenters</span><span class="p">);</span>
    
    <span class="n">thrust</span><span class="o">::</span><span class="n">transform</span><span class="p">(</span><span class="n">thrust</span><span class="o">::</span><span class="n">device</span><span class="p">,</span>
        <span class="n">centers</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">centers</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span>
        <span class="n">old_centers</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span>
        <span class="n">squared_diffs</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span>
        <span class="n">thrust</span><span class="o">::</span><span class="n">minus</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">()</span>
    <span class="p">);</span>
    <span class="n">thrust</span><span class="o">::</span><span class="n">transform</span><span class="p">(</span><span class="n">thrust</span><span class="o">::</span><span class="n">device</span><span class="p">,</span>
        <span class="n">squared_diffs</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">squared_diffs</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span>
        <span class="n">squared_diffs</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span>
        <span class="n">thrust</span><span class="o">::</span><span class="n">square</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">()</span>
    <span class="p">);</span>

    <span class="n">thrust</span><span class="o">::</span><span class="n">reduce_by_key</span><span class="p">(</span>
        <span class="n">d_center_idx</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">d_center_idx</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span>
        <span class="n">squared_diffs</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span>
        <span class="n">thrust</span><span class="o">::</span><span class="n">make_discard_iterator</span><span class="p">(),</span>
        <span class="n">squared_diff_centers</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span>
        <span class="n">thrust</span><span class="o">::</span><span class="n">equal_to</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(),</span>
        <span class="n">thrust</span><span class="o">::</span><span class="n">plus</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">()</span>
    <span class="p">);</span>
<span class="p">}</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <p>The <em>d_center_idx</em> give all the coordinates of a vector the same label so that they are be grouped together in the reduction step.</p> <h4 id="map-points-to-centers-step">Map points to centers step</h4> <p>To find the accumulate sum of points with the same label, for each point with a different centroid label, we give each coordinate of the vector a different, so that we have $\text{number of clusters} \times \text{dimension}$ labels. Suppose point $a_1,\; a_2$ are assigned to centroid 1 and 2 respectively, then we label each coordinate as follows:</p> \[\mathbf{a}_1 = \begin{pmatrix} a_{11} \\ a_{12} \\ \vdots \\ a_{1d} \end{pmatrix} \Rightarrow \begin{pmatrix} \text{label 1}\\ \text{label 2}\\ \vdots \\ \text{label d}\\ \end{pmatrix} ,\hspace{5pt} \mathbf{a}_2 = \begin{pmatrix} a_{21} \\ a_{22} \\ \vdots \\ a_{2d} \end{pmatrix} \Rightarrow \begin{pmatrix} \text{label d+1}\\ \text{label d+2}\\ \vdots \\ \text{label 2d}\\ \end{pmatrix} ,\hspace{5pt}\cdots\] <p>With this, we then use <code class="language-plaintext highlighter-rouge">thrust::stable_sort_by_key</code> to sort the input dataset array of size <code class="language-plaintext highlighter-rouge">(number of points*dimension)</code> by the above labels, then use the labels as key and use <code class="language-plaintext highlighter-rouge">thrust::reduce_by_key</code> to accumulate points with the same labels.</p> <h4 id="calculate-number-of-points-in-each-centroid">Calculate number of points in each centroid</h4> <p>After each point is assigned to its nearest centeroid, the number of points in each centroid can be found by</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td> <td class="rouge-code"><pre><span class="n">thrust</span><span class="o">::</span><span class="n">reduce_by_key</span><span class="p">(</span>
        <span class="n">d_labels</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">d_labels</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> 
        <span class="n">thrust</span><span class="o">::</span><span class="n">constant_iterator</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> 
        <span class="n">thrust</span><span class="o">::</span><span class="n">make_discard_iterator</span><span class="p">(),</span> 
        <span class="n">d_ClusterCounts</span><span class="p">.</span><span class="n">begin</span><span class="p">()</span>
    <span class="p">);</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <h2 id="comparison-between-different-implementations">Comparison between different implementations</h2> <p>We ran the kmeans algorithm on our CPU (serial), basic CUDA, CUDA shared memory and Thrust implementations. For each implementation, we timed and profiled the execution on three different input sets: (1) \texttt{random-n2048-d16-c16.txt} has 2048 points and each point is of dimension 16. (2) \texttt{random-n16384-d24-c16.txt} has 16384 points and each point is of dimension 24. (3) \texttt{random-n65536-d32-c16.txt} has 65536 points and each point is of dimension 32.</p> <p><a id="fig:device1"></a> <img src="/assets/img/kmeans/device1.png" alt="Alt text" width="700"></p> <p>The experiments were conducted on a machine with an AMD Ryzen 5600G CPU (6 cores/12 threads), running Ubuntu 22.04. The GPU used was an Nvidia GTX 1080Ti with 11 GB memory, featuring 3584 CUDA cores, 28 streaming multiprocessors (SMs), and an L1 cache size of 48 KB per SM <d-cite key="gpu"></d-cite>. The CUDA driver version was 545.84, and the CUDA toolkit version was 12.3. Among the implementations, the shared-memory CUDA implementation was the fastest, as expected. This is because using shared memory made the mapPointsToCenters step more efficient. The Nvidia 1080Ti GPU has a maximum of 2048 threads per SM, and with 28 SMs, the maximum number of threads in flight is 57344. This should theoretically represent the maximum speed-up. However, due to variations in the parallelizability of different steps in the k-means algorithm, the actual maximum performance depends on the input size and dimension. From <a href="#fig:speedup">Figure.2 </a>, the maximum speed-up of approximately 5500 was achieved with the shared memory CUDA implementation on the n65536-d32 dataset, which is about one-tenth of the theoretical estimate. The slowest implementation was the Thrust version, which was slower than both the shared-memory and basic CUDA implementations across all input datasets. However, the performance gap between the Thrust and basic CUDA implementations narrowed as input size increased, and for the n65536-d32 dataset, their performance was nearly comparable. It is expected that for larger datasets, the Thrust implementation may outperform the basic CUDA implementation since the distance calculation in the basic version uses a for-loop, while the Thrust implementation is optimized for any dimensionality and is more efficient.</p> <p><a id="fig:speedup"></a> <img src="/assets/img/kmeans/speedup.png" alt="Alt text" width="700"></p> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-10-01-kmeans.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 PoJen Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-kmeans-clustering-algorithm-with-cuda",title:"Kmeans Clustering Algorithm with CUDA",description:"Kmeans Clustering Algorithm with CUDA and CUDA Thrust API",section:"Posts",handler:()=>{window.location.href="/blog/2024/Kmeans/"}},{id:"post-quantum-transfer-learning",title:"Quantum Transfer Learning",description:"Quantum transfer learning with quantum pooling layer",section:"Posts",handler:()=>{window.location.href="/blog/2023/qnn-transfer/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"projects-dataset-artifacts-in-language-models",title:"Dataset Artifacts in Language Models",description:"Mitigating Dataset Artifacts with Adversarial Datasets and Data Cartography",section:"Projects",handler:()=>{window.location.href="/projects/data-artifact/"}},{id:"projects-pytuxkart-ice-hockey-game",title:"PyTuxKart Ice-Hockey Game",description:"Playing PyTuxKart Ice-Hockey Game with Image-Based Agent",section:"Projects",handler:()=>{window.location.href="/projects/pytuxkart/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%70%6A%77%61%6E%67@%70%72%6F%74%6F%6E%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=m5l-w84AAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/nez0b","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/po-jen-wang-49a37855","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/nejopw","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>